# Implementation Plan: Task Operation Tools for AI Agent Integration

**Branch**: `004-mcp-tools` | **Date**: 2026-01-29 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/004-mcp-tools/spec.md`

## Summary

Implement a FastMCP-based server that exposes 5 task management operations (add, list, complete, delete, update) as standardized tools for AI agent integration. The MCP server runs within the FastAPI process, uses the existing SQLModel database layer, and provides stateless tool functions that enforce user isolation. Tools are wrapped with `@function_tool` decorators for OpenAI Agents SDK integration.

**Technical Approach**: Create two modules - `mcp_tools.py` for core MCP tool definitions using FastMCP decorators, and `agent_tools.py` for OpenAI Agent wrapper functions. Tools operate directly on the database via synchronous SQLModel sessions (not async) and return consistent dictionary responses with error handling.

## Technical Context

**Language/Version**: Python 3.13+
**Primary Dependencies**: FastMCP 2.14+, SQLModel 0.0.14+, OpenAI Agents SDK 0.6.5+
**Storage**: Neon PostgreSQL (existing async engine, but tools use sync sessions)
**Testing**: pytest with mocked database sessions for unit tests, MCP Inspector for manual testing
**Target Platform**: Linux server (Render.com deployment)
**Project Type**: Web application (backend component)
**Performance Goals**: <500ms p95 for tool operations, support 100 concurrent tool calls
**Constraints**: Tools must be stateless (no in-memory state), enforce user isolation on all operations
**Scale/Scope**: 5 tools, ~400 LOC total across 2 modules

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Phase 3 AI Agent Requirements ✅

- **AI Agent Framework**: OpenAI Agents SDK 0.6.5+ (confirmed in requirements)
- **Tool Protocol**: FastMCP 2.14+ for Model Context Protocol compliance
- **Tool Decorators**: `@mcp.tool` for MCP definitions, `@function_tool` for agent wrappers
- **Agent Integration**: Tools exported as `ALL_TOOLS` list for agent configuration
- **Stateless Design**: No in-memory state, all operations query database
- **User Isolation**: All tools validate `user_id` ownership before operations

### Core Principles Compliance ✅

- **Type Safety**: All functions have explicit type hints (Python 3.13+ compliance)
- **Clean Architecture**: Tools layer separate from API and service layers
- **Error Handling**: Tools return error dicts (no exceptions raised to agent)
- **Testing**: Unit tests with mocked sessions + manual MCP Inspector testing
- **Documentation**: Complete docstrings with Args/Returns sections

### Additional Gates

- **Dependency Management**: `uv` for package management ✅
- **Code Quality**: Type hints on ALL functions, docstrings required ✅
- **Security**: User ownership validation before all mutations ✅
- **Database**: Uses existing SQLModel models and engine ✅

**Violations**: None - all constitution requirements met.

## Project Structure

### Documentation (this feature)

```text
specs/004-mcp-tools/
├── spec.md              # Feature specification (completed)
├── plan.md              # This file (current)
├── research.md          # Phase 0: MCP patterns, async/sync considerations
├── data-model.md        # Phase 1: Tool signatures, return formats
├── quickstart.md        # Phase 1: Setup guide, testing instructions
├── contracts/           # Phase 1: Tool schemas, error responses
│   ├── mcp-tools.json   # MCP tool definitions schema
│   └── agent-tools.json # Agent wrapper signatures
└── tasks.md             # Phase 2: Generated by /sp.tasks (NOT created here)
```

### Source Code (repository root)

```text
backend/
├── src/
│   ├── mcp_tools.py         # NEW: MCP server and @mcp.tool definitions
│   ├── agent_tools.py       # NEW: OpenAI Agent @function_tool wrappers
│   ├── models/
│   │   ├── task.py          # EXISTING: Task SQLModel (reused)
│   │   └── user.py          # EXISTING: User SQLModel (reused)
│   ├── db/
│   │   └── session.py       # EXISTING: Async engine (need sync session factory)
│   └── api/
│       └── routers/
│           └── chat.py      # FUTURE: Will import agent_tools.ALL_TOOLS
├── tests/
│   ├── unit/
│   │   ├── test_mcp_tools.py      # NEW: Unit tests with mocked DB
│   │   └── test_agent_tools.py    # NEW: Wrapper function tests
│   └── integration/
│       └── test_mcp_integration.py # NEW: End-to-end MCP tool tests
├── requirements.txt         # MODIFY: Add fastmcp>=2.14
└── pyproject.toml          # MODIFY: Add fastmcp to dependencies
```

**Structure Decision**: Web application structure (Option 2). New MCP modules added to `backend/src/` alongside existing models and API. Tools use existing database models and engine configuration. Testing follows existing pytest structure with unit and integration tiers.

## Complexity Tracking

No constitution violations - this section intentionally left empty.

---

## Phase 0: Research

### 0.1 FastMCP Architecture Patterns

**Research Questions**:
1. How does FastMCP integrate with async FastAPI applications?
2. Can MCP tools use async database operations or must they be synchronous?
3. What are FastMCP's session management best practices?
4. How does `@mcp.tool` decorator interact with type hints?

**Deliverable**: `research.md` documenting:
- FastMCP sync vs async tool patterns
- Session management strategies (sync Session vs async AsyncSession)
- Error handling conventions (return dicts vs raise exceptions)
- Tool decorator mechanics and parameter validation

### 0.2 Existing Codebase Integration

**Research Questions**:
1. How is the current async database engine configured? (Check `src/db/session.py`)
2. What is the Task model schema? (Check `src/models/task.py`)
3. How does the existing service layer handle user isolation?
4. Are there existing utility functions for session management?

**Deliverable**: `research.md` section documenting:
- Current async engine configuration and pool settings
- Task model fields and relationships
- User isolation patterns in existing code
- Whether to create sync session factory or adapt existing async patterns

### 0.3 OpenAI Agents SDK Integration

**Research Questions**:
1. What is the `@function_tool` decorator signature?
2. How does the agent invoke tools at runtime?
3. What format does the agent expect for tool results?
4. How are errors communicated to the agent?

**Deliverable**: `research.md` section documenting:
- `@function_tool` decorator usage and requirements
- Tool invocation flow (agent → wrapper → MCP tool → database)
- Expected return formats and error conventions
- Agent error handling mechanisms

---

## Phase 1: Design

### 1.1 Tool Signatures and Return Formats

**Design Decision**: All tools accept `user_id: str` as first parameter for user isolation. All tools return `dict` with consistent structure.

**Standard Return Format**:
```python
# Success
{
    "task_id": int,
    "status": "created" | "completed" | "updated" | "deleted",
    "title": str
}

# Error
{
    "error": str  # Human-readable error message
}

# List operation
[
    {
        "id": int,
        "title": str,
        "description": str | None,
        "completed": bool
    },
    ...
]
```

**Deliverable**: `data-model.md` documenting:
- Complete function signatures for all 5 tools
- Return type schemas with examples
- Error response formats
- Validation rules (title length, user_id format, etc.)

### 1.2 Database Session Strategy

**Design Decision**: Use synchronous `Session` from SQLModel for MCP tools (not async `AsyncSession`).

**Rationale**:
- FastMCP tools can be sync or async; sync is simpler for database operations
- Existing async engine can be used with sync sessions via `Session(engine)`
- Avoids async/await complexity in tool functions
- FastAPI async routes remain unaffected (tools called from agent, not directly from routes)

**Pattern**:
```python
from sqlmodel import Session
from src.db.session import engine  # Reuse existing async engine

@mcp.tool
def tool_name(user_id: str, ...) -> dict:
    with Session(engine) as session:
        # Database operations
        session.commit()
        return result
```

**Deliverable**: `data-model.md` section documenting:
- Session context manager pattern
- Engine reuse strategy
- Transaction commit/rollback handling
- Connection pooling considerations

### 1.3 User Isolation and Security

**Design Decision**: All tools validate user ownership before mutation operations.

**Validation Pattern**:
```python
task = session.get(Task, task_id)
if not task or task.user_id != user_id:
    return {"error": f"Task {task_id} not found"}
```

**Security Requirements**:
- Never allow cross-user data access
- Return same error message for "not found" and "access denied" (prevent user enumeration)
- Validate all parameters before database queries
- Use parameterized queries (SQLModel handles this automatically)

**Deliverable**: `data-model.md` section documenting:
- User ownership validation logic
- Error message standards for security
- Parameter validation requirements
- SQL injection prevention (via SQLModel ORM)

### 1.4 Error Handling Strategy

**Design Decision**: Tools never raise exceptions; always return error dicts.

**Error Categories**:
1. **Validation Errors**: Invalid parameters (empty title, negative task_id, etc.)
2. **Not Found Errors**: Task doesn't exist or user doesn't own it
3. **Database Errors**: Unexpected database failures

**Error Response Pattern**:
```python
# Input validation
if not title or not title.strip():
    return {"error": "Title is required"}

# Ownership validation
if not task or task.user_id != user_id:
    return {"error": f"Task {task_id} not found"}

# Unexpected errors (catch-all)
try:
    # ... operation ...
except Exception as e:
    # Log error internally
    logger.error(f"Error in tool_name: {e}")
    return {"error": "An unexpected error occurred"}
```

**Deliverable**: `contracts/mcp-tools.json` documenting:
- Error response schema
- Error message templates
- Logging requirements for internal errors
- Examples of each error category

### 1.5 MCP Server Configuration

**Design Decision**: Create single MCP server instance in `mcp_tools.py` module.

**Configuration**:
```python
from fastmcp import FastMCP

mcp = FastMCP(
    name="Hackathon Todo MCP Server",
    dependencies=["sqlmodel", "asyncpg"]  # Declare dependencies
)
```

**Tool Registration**: Use `@mcp.tool` decorator for automatic registration.

**Standalone Testing**: Support `fastmcp dev` for interactive testing.

**Deliverable**: `data-model.md` section documenting:
- MCP server configuration options
- Tool registration mechanics
- Development/testing workflow with MCP Inspector
- Integration with FastAPI application

### 1.6 Agent Wrapper Layer

**Design Decision**: Create separate `agent_tools.py` module with `@function_tool` wrappers.

**Wrapper Pattern**:
```python
from agents import function_tool
from src.mcp_tools import add_task as mcp_add_task

@function_tool
def add_task(user_id: str, title: str, description: str = None) -> dict:
    """
    Add a new task to the user's todo list.

    Args:
        user_id: The unique identifier of the user
        title: The title of the task (required)
        description: Optional description with more details

    Returns:
        Dictionary containing task_id, status, and title
    """
    return mcp_add_task(user_id, title, description)
```

**Rationale**:
- Separates MCP concerns from agent concerns
- Allows independent testing of MCP tools
- Provides single point of tool export (`ALL_TOOLS` list)
- Maintains identical signatures and docstrings

**Deliverable**: `contracts/agent-tools.json` documenting:
- Agent wrapper signatures
- ALL_TOOLS export list structure
- Integration with OpenAI Agent
- Docstring requirements for agent tool discovery

---

## Phase 1 Deliverables Summary

By end of Phase 1, the following artifacts will be created:

1. **research.md**: FastMCP patterns, codebase integration analysis, Agents SDK integration
2. **data-model.md**: Tool signatures, session strategy, security patterns, error handling, MCP config
3. **quickstart.md**: Developer setup guide, testing workflow, MCP Inspector usage
4. **contracts/mcp-tools.json**: MCP tool schemas and error formats
5. **contracts/agent-tools.json**: Agent wrapper schemas and export structure

These artifacts provide complete technical specification for implementation phase (`/sp.tasks`).

---

## Phase 2: Implementation Planning (NOT IN THIS DOCUMENT)

Phase 2 will be executed by `/sp.tasks` command, which generates `tasks.md` with:

- **Dependency Installation**: Add fastmcp to requirements
- **MCP Tools Module**: Implement 5 tools with @mcp.tool decorators
- **Agent Tools Module**: Implement 5 wrappers with @function_tool decorators
- **Unit Tests**: Test each tool with mocked database
- **Integration Tests**: End-to-end MCP tool testing
- **MCP Inspector Testing**: Manual validation checklist

Each task will include:
- Acceptance criteria with specific validations
- Code references to existing models and session management
- Test cases for all scenarios (success, errors, edge cases)

---

## Key Architectural Decisions

### ADR-001: Use Synchronous Sessions for MCP Tools

**Context**: MCP tools need database access; existing codebase uses async SQLAlchemy.

**Decision**: Use synchronous `Session(engine)` within MCP tools instead of async sessions.

**Rationale**:
- FastMCP supports both sync and async tools; sync is simpler
- Existing async engine works with sync sessions
- Tools are invoked by agent (not directly from async routes), so no async propagation needed
- Reduces complexity and potential async/await bugs

**Consequences**:
- Tools use `with Session(engine)` context manager
- No `await` keywords in tool functions
- Existing async routes and services unaffected
- Slightly higher latency (blocking I/O) but negligible for single-user tool calls

**Alternatives Considered**:
- **Async tools with AsyncSession**: Rejected due to complexity and unnecessary async propagation
- **Separate sync engine**: Rejected due to duplication and connection pool waste

---

### ADR-002: Separate MCP and Agent Tool Layers

**Context**: Tools must work with both FastMCP (for testing) and OpenAI Agents SDK (for production).

**Decision**: Create two modules - `mcp_tools.py` with `@mcp.tool` and `agent_tools.py` with `@function_tool` wrappers.

**Rationale**:
- Allows independent testing of MCP tools via `fastmcp dev`
- Separates MCP protocol concerns from agent framework concerns
- Provides single point of tool export for agent configuration
- Maintains clean separation of concerns

**Consequences**:
- Two files instead of one (slight duplication)
- Wrappers simply delegate to MCP tools (thin layer)
- MCP tools testable without agent framework
- Agent configuration imports from `agent_tools.ALL_TOOLS`

**Alternatives Considered**:
- **Single file with dual decorators**: Rejected due to decorator conflicts and coupling
- **Direct MCP tool usage in agent**: Rejected due to MCP server dependency in production

---

### ADR-003: Return Dicts with Error Keys Instead of Exceptions

**Context**: Tools can fail for various reasons (validation, not found, database errors).

**Decision**: Tools always return `dict`, using `{"error": "message"}` for failures instead of raising exceptions.

**Rationale**:
- Agent can handle error responses gracefully and formulate user-friendly messages
- Prevents agent crashes from uncaught exceptions
- Consistent return type simplifies agent tool integration
- Allows agent to retry or ask for clarification

**Consequences**:
- All tool calls must check for "error" key in response
- No Python exception handling needed in agent
- Internal errors logged but not exposed to agent
- Success and error paths use same return type

**Alternatives Considered**:
- **Raise HTTPException**: Rejected as tools aren't HTTP endpoints
- **Raise custom exceptions**: Rejected as agents don't handle Python exceptions well
- **Status codes in response**: Considered but dict with error key is simpler

---

## Testing Strategy

### Unit Tests (60% coverage target)

**Scope**: Individual tool functions with mocked database sessions

**Approach**:
```python
import pytest
from unittest.mock import Mock, patch
from src.mcp_tools import add_task

def test_add_task_success():
    """Test successful task creation."""
    mock_session = Mock()
    mock_task = Mock(id=1, title="Test", user_id="user123")

    with patch('src.mcp_tools.Session') as mock_session_cls:
        mock_session_cls.return_value.__enter__.return_value = mock_session
        mock_session.add = Mock()
        mock_session.commit = Mock()
        mock_session.refresh = Mock(side_effect=lambda t: setattr(t, 'id', 1))

        result = add_task("user123", "Test task")

        assert result == {
            "task_id": 1,
            "status": "created",
            "title": "Test task"
        }
        mock_session.add.assert_called_once()
        mock_session.commit.assert_called_once()

def test_add_task_empty_title():
    """Test validation error for empty title."""
    # Add validation to tool first
    result = add_task("user123", "")
    assert "error" in result
```

**Test Coverage**:
- Success cases for all 5 tools
- Validation errors (empty title, invalid task_id, etc.)
- Not found errors (task doesn't exist, wrong user)
- Edge cases (None description, multiple tasks, etc.)

### Integration Tests (30% coverage target)

**Scope**: End-to-end tool execution with real database

**Approach**:
```python
import pytest
from sqlmodel import Session, create_engine
from src.mcp_tools import add_task, list_tasks
from src.models import Task

@pytest.fixture
def test_db():
    """Create test database."""
    engine = create_engine("sqlite:///:memory:")
    # Create tables
    yield engine

def test_full_workflow(test_db):
    """Test complete add -> list -> complete -> delete workflow."""
    # Add task
    result = add_task("user123", "Integration test")
    assert result["status"] == "created"
    task_id = result["task_id"]

    # List tasks
    tasks = list_tasks("user123", "all")
    assert len(tasks) == 1
    assert tasks[0]["id"] == task_id

    # ... complete and delete ...
```

### Manual Testing (10% coverage target)

**MCP Inspector Workflow**:
```bash
cd backend
fastmcp dev src/mcp_tools.py
# Browser opens at http://127.0.0.1:6274
# Test each tool interactively
```

**Test Checklist**:
- [ ] add_task creates task and returns task_id
- [ ] list_tasks with "all" returns all tasks
- [ ] list_tasks with "pending" filters correctly
- [ ] complete_task marks task as done
- [ ] update_task modifies title/description
- [ ] delete_task removes task
- [ ] Ownership validation prevents cross-user access
- [ ] Error responses return dict with "error" key

---

## Dependencies

### New Dependencies

Add to `backend/requirements.txt` and `backend/pyproject.toml`:

```toml
[project]
dependencies = [
    # ... existing dependencies ...
    "fastmcp>=2.14",
]
```

**Version Constraints**:
- `fastmcp>=2.14`: Required for `@mcp.tool` decorator and MCP Inspector
- Compatible with existing SQLModel and FastAPI versions

### Existing Dependencies (reused)

- `sqlmodel>=0.0.14`: Database ORM (already installed)
- `asyncpg>=0.29.0`: PostgreSQL async driver (already installed)
- `python-jose[cryptography]`: JWT handling (not used in tools, but available)

### Development Dependencies

- `pytest>=8.0.0`: Unit and integration testing (already installed)
- `pytest-asyncio>=0.23.0`: Async test support (already installed)
- No new dev dependencies required

---

## Risks and Mitigations

### Risk 1: Async Engine with Sync Sessions

**Risk**: Using async engine with synchronous `Session()` might cause connection pool issues or deadlocks.

**Likelihood**: Low
**Impact**: High (production database failures)

**Mitigation**:
1. Test with real database in integration tests
2. Monitor connection pool usage during load testing
3. Document session pattern clearly in code comments
4. Add connection pool metrics to monitoring

**Contingency**: If issues arise, create separate sync engine for MCP tools.

### Risk 2: Tool Performance Under Load

**Risk**: Synchronous tools might block event loop if agent makes many concurrent tool calls.

**Likelihood**: Medium
**Impact**: Medium (slower response times)

**Mitigation**:
1. Set performance goals (<500ms p95) and test against them
2. Use database query optimization (indexes, select only needed fields)
3. Run load tests with 100 concurrent tool calls
4. Monitor p95/p99 latency in production

**Contingency**: Convert to async tools if latency exceeds acceptable thresholds.

### Risk 3: Error Handling Inconsistency

**Risk**: Different tools might return errors in different formats, confusing the agent.

**Likelihood**: Low (prevented by good testing)
**Impact**: High (agent failures, poor UX)

**Mitigation**:
1. Define standard error format in contracts/mcp-tools.json
2. Create error response helper function
3. Test error paths explicitly in unit tests
4. Review all error messages for consistency

**Contingency**: Refactor error responses if inconsistencies discovered in testing.

---

## Success Metrics

### Phase 0 Success Criteria

- [ ] Research document answers all key questions
- [ ] Async vs sync strategy decided with clear rationale
- [ ] Existing codebase integration points identified
- [ ] No blockers identified requiring spec changes

### Phase 1 Success Criteria

- [ ] All 5 tool signatures defined with types
- [ ] Return format schema documented with examples
- [ ] Session management pattern documented
- [ ] Error handling strategy documented
- [ ] MCP server configuration specified
- [ ] Agent wrapper pattern specified
- [ ] Contracts JSON files created and validated

### Phase 2 Success Criteria (for /sp.tasks)

- [ ] All 5 MCP tools implemented with @mcp.tool
- [ ] All 5 agent wrappers implemented with @function_tool
- [ ] Unit tests achieve 60%+ coverage
- [ ] Integration tests pass with real database
- [ ] MCP Inspector manual testing checklist completed
- [ ] No type errors from mypy
- [ ] No linting errors from ruff

### Production Success Criteria

- [ ] Tools execute in <500ms p95 latency
- [ ] Zero cross-user data access incidents
- [ ] Agent successfully calls all 5 tools
- [ ] Error responses handled gracefully by agent
- [ ] No database connection pool exhaustion

---

## Next Steps

1. **Execute Phase 0**: Run `/sp.plan` command to generate `research.md`
2. **Execute Phase 1**: Continue `/sp.plan` to generate design artifacts
3. **Review Plan**: Validate with team/stakeholder before moving to tasks
4. **Execute Phase 2**: Run `/sp.tasks` to generate implementation tasks
5. **Implement**: Follow tasks.md with TDD approach
6. **Test**: Run unit, integration, and manual MCP Inspector tests
7. **Deploy**: Add to production agent configuration

---

## Appendix A: File Modification Summary

| File | Change Type | Description |
|------|-------------|-------------|
| `backend/src/mcp_tools.py` | **CREATE** | MCP server and 5 @mcp.tool functions |
| `backend/src/agent_tools.py` | **CREATE** | 5 @function_tool wrappers and ALL_TOOLS export |
| `backend/requirements.txt` | **MODIFY** | Add `fastmcp>=2.14` |
| `backend/pyproject.toml` | **MODIFY** | Add `fastmcp>=2.14` to dependencies |
| `backend/tests/unit/test_mcp_tools.py` | **CREATE** | Unit tests for MCP tools |
| `backend/tests/unit/test_agent_tools.py` | **CREATE** | Unit tests for agent wrappers |
| `backend/tests/integration/test_mcp_integration.py` | **CREATE** | Integration tests |

**Total**: 4 new files, 2 modified files

---

## Appendix B: Tool Signatures Reference

```python
# MCP Tools (src/mcp_tools.py)

@mcp.tool
def add_task(user_id: str, title: str, description: str | None = None) -> dict:
    """Create new task."""

@mcp.tool
def list_tasks(user_id: str, status: str = "all") -> list[dict]:
    """Retrieve tasks with optional status filter."""

@mcp.tool
def complete_task(user_id: str, task_id: int) -> dict:
    """Mark task as completed."""

@mcp.tool
def update_task(
    user_id: str,
    task_id: int,
    title: str | None = None,
    description: str | None = None
) -> dict:
    """Update task title and/or description."""

@mcp.tool
def delete_task(user_id: str, task_id: int) -> dict:
    """Delete task permanently."""

# Agent Tools (src/agent_tools.py)
# Same signatures with @function_tool decorator
```

---

**Plan Status**: ✅ Phase 0 and Phase 1 specifications complete. Ready for `/sp.tasks` command.
